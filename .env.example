# .env.example file, Note:don't include any value to variable in string, provided as it is

#Used GROQ PROVIDER: LLM model should be spported by groq,else throws error
LLM_MODEL_NAME=

GROQ_API_KEY=

# used FastEmbedEmbeddings library, please ensure the embedding model should be supported.. else throws error
DENSE_EMBEDDING_MODEL=

#index parameters for efficient retrieval from milvusdb
DENSE_SEARCH_INDEX_TYPE=IVF_SQ8
DENSE_SEARCH_METRIC_TYPE=L2
DENSE_SEARCH_NLIST=128

# used SparseFastEmbedEmbeddings library, please ensure the embedding model should be supported.. else throws error
SPARSE_EMBEDDING_MODEL=

#index parameters for efficient retrieval from milvusdb
SPARSE_SEARCH_INDEX_TYPE=SPARSE_INVERTED_INDEX
SPARSE_SEARCH_METRIC_TYPE=IP

#OpenAI API base for RAGAS Evaluation e.g https://api.aimlapi.com/v1
OPENAI_API_BASE=

#Model parameters to set
TEMPERATURE=0.0
TOP_P=0.3
FREQUENCY_PENALTY=1.0

#Chabot Memory
CHAT_HISTORY=2

IS_EVALUATE=True

IS_RERANK=False
RERANK_TOPK=3

#Add Guardrails for contolled responses, detect prompt injestions, safety, greetings etc.
IS_GUARDRAIL=False

LANGCHAIN=True

#If Not Provided, default_value provided in the codebase
QUESTION_MODERATION_PROMPT=

#If Not Provided, default_value provided in the codebase
MASTER_PROMPT=

#If provided model as llama, then only pass the values.. If not provided, passed llama3.1 default values
LLAMA3_USER_TAG=
LLAMA3_SYSTEM_TAG=
LLAMA3_ASSISTANT_TAG=

#Sample, Module initiator..
QUESTION=What is Generative AI?

#Optional: Streamlit and RestAPI specific, to securely loads and decrypts the credentials/API keys.
CONNECTION_STRING=
MONGO_COLLECTION_NAME=
DB_NAME=

#Optional: Streamlit specific, to store the question, answer inside github as chat_history
GITHUB_REPO_NAME=
CHATFILE_PATH=

#Zillinz Cloud credentials for Milvus VectoDB
ZILLIZ_CLOUD_URI=
ZILLIZ_CLOUD_API_KEY=
COLLECTION_NAME=

#Parameters for TopK_Results
HYBRID_SEARCH_TOPK=6

