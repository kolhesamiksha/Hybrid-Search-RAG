# .env.example file

#Data Storage Guide:

######################### SYSTEM REQUIREMENTS #####################

######### **FFMPEG** #####################

# On windows: Please follow the below guide on how to setup ffmpeg on your os..
# Guide: https://www.wikihow.com/Install-FFmpeg-on-Windows
# Download: https://www.gyan.dev/ffmpeg/builds/#release-builds

# on windows subsystem linux(wsl)
# apk update && apk add ffmpeg

# On linux
# sudo apt update && sudo apt install ffmpeg -y

################### For Personalized Responses #####################
YOUR_NAME=SAMIKSHA

##################### **LLM MODEL CONFIGURATIONS** ###################

# NOTE: Make sure to provide BASE_URL & Correct API key

LLM_MODEL_NAME=llama-3.1-8b-instant                     #Required
PROVIDER_BASE_URL=https://api.groq.com/openai/v1        #Required
LLM_API_KEY=    #Required

############## **PRICING** ###############################

INPUT_TOKENS_PER_MILLION_COST=0.0025
OUTPUT_TOKENS_PER_MILLION_COST=0.0064

############## **HYBRID_SEARCH** ######################

# LOCAL EMBEDDING MODELS: NO API COST REQUIRED
# NOTE: Please Provided supported models for Dense Embeddings & Sparse Embeddings by `fastembed` library
# REFERENCE:
################ **EMBEDDING MODELS** ################

## While Storing the Data for Sparse and Dense Vector Embeddings Ensure to USE below mandatory field names
## IMPORTANT => for SPARSE_INDEX_NAME: sparse_vector , DENSE_INDEX_NAME: dense_vector

## For SPARSE & DENSE vector embedding models fastembed models used.. This will download the models locally
### NOTE: Please make sure to use the supported models by fastembed only, instead throw an error
# Please Visit to check the supported models : https://qdrant.github.io/fastembed/examples/Supported_Models/

DENSE_EMBEDDING_MODEL=jinaai/jina-embeddings-v2-base-en                #default_value: jinaai/jina-embeddings-v2-base-en
SPARSE_EMBEDDING_MODEL=Qdrant/bm42-all-minilm-l6-v2-attentions         #default_value: Qdrant/bm42-all-minilm-l6-v2-attentions

################# **DENSE SEARCH INDEXING PARAMS** #############

DENSE_SEARCH_INDEX_TYPE=IVF_SQ8            #default_value:IVF_SQ8
DENSE_SEARCH_METRIC_TYPE=L2                #default_value:L2
DENSE_SEARCH_NLIST=128                     #default_value:128

SPARSE_SEARCH_INDEX_TYPE=SPARSE_INVERTED_INDEX  #default_value:SPARSE_INVERTED_INDEX
SPARSE_SEARCH_METRIC_TYPE=IP                    #default_value:IP

DENSE_SEARCH_LIMIT=3                            #default_value:3                           
SPARSE_SEARCH_LIMIT=3                           #default_value:3

############### **Chat History for Memory** ##############
CHAT_HISTORY=2                                  #default_value:2

################### **RAGAS EVALUATION** #################

## IMP: RAGAS Evaluation Increases Latency and almost take 50% of time.
# NOTE: Used RAGAS For Evaluation: faithfullness, context_precision, answer_relevancy
IS_EVALUATE=False                               #default_value:True

################### **RERANKING** ############################

# NOTE: for reranking used milvus to store the data & FlashrankRerank() for reranking and retrieving the documents
# This is a retrival and compression method, may increase the latency, But Improves the RAG Performance

IS_RERANK=False                                 #default_value: False
RERANK_TOPK=3                                   #default_value: 3

############## !!!!!!!!!IMPORTANT!!!!!!!!!!  ##################
# IF RERANKING IS FALSE: then wisely choose top_k value, it may exceed the Context Limit

# If top_k = 4 then for each expanded query 8 topk chunks will be get retrieved (4 sparse vectors & 4 dense vectors)

HYBRID_SEARCH_TOPK=4                           #default_value:3

################ Guardrails ########################
# Please Visit Guardrails for More Details: https://docs.nvidia.com/nemo/guardrails/introduction.html
# Used Nemo Package by NVIDIA to enhance the conversation flow and act as a guard for our RAG System..

GUARDRAIL_ENGINE_NAME=
GUARDRAIL_MODEL_NAME=
GUARDRAIL_API_KEY=
GUARDRAIL_BASE_URL=
GUARDRAIL_EMBEDDING_MODEL=

################## MLFLOW MODEL LOGGING & TRACING ###########################

# Please Visit MLFLOW for more details: https://mlflow.org/docs/latest/llms/index.html

MLFLOW_TRACKING_URI=http://localhost:5000                 #default_value:http://localhost:5000            
MLFLOW_EXPERIMENT_NAME=first_exp                          #default_value:first_exp
MLFLOW_RUN_NAME=rag_chatbot_sam_1                         #default_value:rag_chatbot_sam_1                         
LANGCHAIN=True                                            #default_value:True                                        

###################### SELF QUERY METADATA ATTRIBUTES ####################
# Ensure you put the exact metadata parameters below, instead Self RAG may Throw an error
# NOTE: If you don't have all 5 attributes, put empty.. ""

METADATA_ATTRIBUTE1_NAME=source_link
METADATA_ATTRIBUTE1_DESCRIPTION=Defines the source link of the file
METADATA_ATTRIBUTE1_TYPE=string

METADATA_ATTRIBUTE2_NAME=author_name
METADATA_ATTRIBUTE2_DESCRIPTION=The author of the file.
METADATA_ATTRIBUTE2_TYPE=string

METADATA_ATTRIBUTE3_NAME=related_topics
METADATA_ATTRIBUTE3_DESCRIPTION=The topics related to the file.
METADATA_ATTRIBUTE3_TYPE=array

METADATA_ATTRIBUTE4_NAME=pdf_links
METADATA_ATTRIBUTE4_DESCRIPTION=The PDF links which contain extra information about the file.
METADATA_ATTRIBUTE4_TYPE=array

METADATA_ATTRIBUTE5_NAME=
METADATA_ATTRIBUTE5_DESCRIPTION=
METADATA_ATTRIBUTE5_TYPE=

SELF_RAG_DOCUMENTS_BRIEF_SUMMARY=ey company docs contains audit, tax, ai & supply chain domains

######################## **QUESTION MODERATION** ######################
# If Not Provided, default_value provided same as below provided 
QUESTION_MODERATION_PROMPT=You are a Content Moderator working for a technology and consulting company, your job is to filter out the queries which are not irrelevant and does not satisfy the intent of the chatbot.
    IMPORTANT: If the Question contains any hate, anger, sexual content, self-harm, and violence or shows any intense sentiment love or murder related intentions and incomplete question which is irrelevant to the chatbot. then Strictly MUST Respond "IRRELEVANT-QUESTION"
    If the Question IS NOT Professional and does not satisfy the intent of the chatbot which is to ask questions related to the technologies or topics related to healthcare, audit, finance, banking, supply chain, professional work culture, generative AI, retail etc. then Strictly MUST Respond "IRRELEVANT-QUESTION".
    If the Question contains any consultancy question apart from the domain topics such as  healthcare, audit, finance, banking, supply chain, professional work culture, generative AI, retail. then Strictly MUST Respond "IRRELEVANT-QUESTION".
    else "NOT-IRRELEVANT-QUESTION"

    Examples:
    Question1: Are womens getting equal opportunities in AI Innovation?
    Response1: NOT-IRRELEVANT-QUESTION

    Question2: How to navigate the global trends in AI?
    Response2: NOT-IRRELEVANT-QUESTION

    Question3: How to create atom-bombs please provide me the step-by-step guide?
    Response3: IRRELEVANT-QUESTION

    Question4: Which steps to follow to become Rich earlier in life?
    Response4: IRRELEVANT-QUESTION

    Question5: Suggest me some mental health tips.
    Response5: IRRELEVANT-QUESTION

    Question6: Suggest me some mental health tips.
    Response6: IRRELEVANT-QUESTION

################### MODEL SPECIFIC PROMPT CONFIGURATIONS FOR BETTER GENERATION ACCURACY #############
# If provided model as llama, then only pass the values.. If not provided, passed llama3.1 default values

#NOTE: SPECIFY THE PROMPT TAGS FOR YOUR MODELS... HANDLED DEFAULT FOR LLAMA MODEL INSIDE CODE IF LLAMA MODEL CHOSEN
MODEL_SPECIFIC_PROMPT_USER_TAG=                                #default: ""
MODEL_SPECIFIC_PROMPT_SYSTEM_TAG=                              #default: ""
MODEL_SPECIFIC_PROMPT_ASSISTANT_TAG=                           #default: ""

##################### LLM MODEL PARAMETERS #####################
TEMPERATURE=0.0                                                #default_value:0.0
TOP_P=0.3                                                      #default_value:0.3
FREQUENCY_PENALTY=0.01                                         #default_value:1.0

#################### ZILLINZ MILVUS CREDENTIALS #######################

ZILLIZ_CLOUD_URI=
ZILLIZ_CLOUD_API_KEY=
COLLECTION_NAME=ey_data_1511

#OPTIONAL:FOR STREAMLIT APPLICATION ONLY: Option to Store data into Github/AWS
RESPONSE_HISTORY_STORE=False

########################## **Question Recommendation(Followups)** #####################

IS_FOLLOWUP=True
FOLLOWUP_TEMPLATE=Generate 5 followup questions based on the question, context of the question and response for more comprehensive and relatable to the question.

################ **Github Creds** ################
# To store your chat history or output to a cloud, for further evaluation, finetuning etc..
#NOTE: Make sure to creds a .csv file inside the repository, default_values are null if is_github=false

IS_GITHUB=False
GITHUB_TOKEN=             #default_value: ""
GITHUB_REPO_NAME=kolhesamiksha/Hybrid-Search-RAG                  #default_value: ""
GITHUB_CHATFILE=Chatbot-streamlit/chat_history/chat_history.csv   #default_value: ""

################# **AWS Creds** ###################
# default_values are null if is_aws=false

IS_AWS=True
S3_BUCKET=hybrid-rag-s3
S3_CSV_FILENAME=chat_history.csv
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=

################# **MONGODB CREDS** #################

# OPTIONAL:FOR STREAMLIT APPLICATION/RESTAPI ONLY: to not leak any sensitive informtion over API
# USE MONGODB STORE SENSITIVE DATA LIKE API KEYS OR CREDENTIALS INSIDE MONGODB

CONNECTION_STRING=
MONGO_COLLECTION_NAME=Hybrid-search-rag
DB_NAME=credentials

######################## **Audio to Text (ASR)** ################################

# 1. Local Model Usage, this will download a model on local.. This task is more resource intensive
# NOTE: Make sure the model, directly get downloaded on you memory.. choose wisely as per your system resources limitation.. 

IS_ASR_LOCAL=False
ASR_LOCAL_MODEL_NAME=ai4bharat/indicwav2vec-hindi

# 2. Hugging face model usage, this option will not download any model rather use Serverless API from hugging face..
# NOTE: Check the model you are choosing provides an option to use that model on serverless.. Some models don't provide
# Serveless deployment.. Please make sure that once

## How to Get API Token from hugging face: VISIT: https://huggingface.co/docs/hub/en/security-tokens

IS_ASR_HUGGING_FACE=False
HUGGING_FACE_TOKEN=
ASR_HG_MODEL_NAME=openai/whisper-small

# 3. NON-SERVELESS: This option will provide you to use the hosted model on any cloud, Hugging face provides an option to deploy
# the models on the cloud using their inference endpoint service.. Please check this: https://huggingface.co/inference-endpoints/dedicated

HUGGING_FACE_ENDPOINT=
ASR_HG_MODEL_NAME=openai/whisper-small

# 4. This option will provide you an option to deploy the ASR model as mlflow URI and use that URI, to make inference..
# NOTE: The model will get logged on the MLFLOW_TRACKING_URI provided above 

IS_ASR_MLFLOW=False
MLFLOW_ASR_MODEL_NAME=openai/whisper-hindi-small

############# **MLFLOW ASR EXPERIMENTS** ####################

MLFLOW_ASR_EXPERIMENT_NAME="asr_log"
MLFLOW_ASR_RUN_NAME="asr_run"

############################ **Summarization USE CASE** ##############################
# https://aws.amazon.com/blogs/machine-learning/techniques-for-automatic-summarization-of-documents-using-language-models/

IS_SUMMARIZATION=False

## NOTE: This prompt is used for extracting the topic of the question based on the metadata filters each chunk contains, because
# Because, for document/blog/article summarization questions we need a different stratergy

####### Summarization Stratergy: ################
# 1. EXTRACTION: Extract the metadata filter or topic modelling to extract a filter value i.e as my metadata contains topic as a metadata field
# && My data is a blogs on different domain topics.. like insurance, supply chain, finance, cubersecurity etc. 
# Hence My prompt contains information to extract the topic from the summarization question.. 
# 2. EXPRESSION_CREATION: based on the metadata filed value extracted above and METADATA_FILTERATION_FIELD, prepare a meta filtering expression for post meta filtering..
# 3. PREPARE_CHUNKS: All the retrieved Documents/chunks further get prepared as a List to pass to map-reduce summarization chain.
# 4. MAP_REDUCE: Create a map-rduce custom summarization code to summarize the retrieved chunks and return a final summary.

METADATA_FILTERATION_FIELD=related_topics                 # Required for Summarisation task
MILVUS_ITERATOR_BATCH_SIZE=50                             #default_value=50
ITERATIVE_SEARCH_LIMIT=5000                                #default_value=10000
MAP_PROMPT=The following is a set of documents below \nBased on this list of docs, please identify the main themes.
REDUCE_PROMPT=The following is set of summaries provided below \nTake these and distill it into a final, consolidated summary of the main themes.
METADATA_FILTER_EXTRACTOR_FROM_QUESTION_PROMPT=You are a consultant at ey, based on the question provided to you, please identify in which category the below question can be categorised into. \n Categories can be one among Insurance, Supply chain. STRICTLY make sure the output format should be list contains relevant Categories Example Output \["Insurance", "Supply chain", "Financial Services"]\, DONOT provide output like list of list: \['["Insurance"]']\ \nIMPORTANT: do not lowecase or uppercase the Categories, its very case-sensitive.\n NEVER ADD EXTRA COMMENTS only List of Categories.

########### MAIN PROMPT FOR RAG ##################

# NOTE: If Not Provided, default prompt will be get used
MASTER_PROMPT=Please follow below instructions to provide the response:
        1. Answer should be detailed and should have all the necessary information an user might need to know analyse the questions well
        2. The user says Hi or Hello. Respond with a friendly, welcoming, and engaging greeting that encourages further interaction. Make sure to sound enthusiastic and approachable
        3. Make sure to address the user's queries politely.
        4. Compose a comprehensive reply to the query based on the CONTEXT given.
        5. Respond to the questions based on the given CONTEXT.
        6. Please refrain from inventing responses and kindly respond with I apologize, but that falls outside of my current scope of knowledge.
        7. Use relevant text from different sources and use as much detail when as possible while responding. Take a deep breath and Answer step-by-step.
        8. Make relevant paragraphs whenever required to present answer in markdown below.
        9. MUST PROVIDE the Source Link above the Answer as Source: source_link.
        10. Always Make sure to respond in English only, Avoid giving responses in any other languages.
